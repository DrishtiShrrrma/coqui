{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCS2atfAJ1Sb"
      },
      "outputs": [],
      "source": [
        "!pip install TTS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "DUbgCLgWTFDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BaseDatasetConfig: defines name, formatter and path of the dataset.\n",
        "from TTS.tts.configs.shared_configs import BaseDatasetConfig"
      ],
      "metadata": {
        "id": "QHL5iARoc9Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BaseDatasetConfig provides a configuration template for managing dataset-related settings in text-to-speech (TTS) applications"
      ],
      "metadata": {
        "id": "ZfOYff9dno_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"tts_train_dir\"\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)"
      ],
      "metadata": {
        "id": "dpKnfrz2ny7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O tts_train_dir/LJSpeech-1.1.tar.bz2 https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2 "
      ],
      "metadata": {
        "id": "OcFYNNWnoqj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf tts_train_dir/LJSpeech-1.1.tar.bz2"
      ],
      "metadata": {
        "id": "wsP_dRobqMwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_config = BaseDatasetConfig(\n",
        "    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=\"/content/LJSpeech-1.1\")"
      ],
      "metadata": {
        "id": "ObxUcNy1qbs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GlowTTSConfig: all model related values for training, validating and testing.\n",
        "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
        "config = GlowTTSConfig(\n",
        "    batch_size=32,\n",
        "    eval_batch_size=16,\n",
        "    num_loader_workers=4,\n",
        "    num_eval_loader_workers=4,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=-1, #This value means that testing will be performed immediately without any delay after each epoch of training. The value -1 indicates that there is no specific number of epochs to wait before starting the testing phase.\n",
        "    epochs=100,\n",
        "    text_cleaner=\"phoneme_cleaners\",\n",
        "    use_phonemes=True,\n",
        "    phoneme_language=\"en-us\",\n",
        "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
        "    print_step=25,\n",
        "    print_eval=False,\n",
        "    mixed_precision=True,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        "    save_step=1000,\n",
        ")"
      ],
      "metadata": {
        "id": "Uv3DYidkrQ3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: A text cleaner is responsible for performing various text normalization operations to convert the raw input text into a cleaner representation suitable for TTS processing. The \"phoneme_cleaners\" text cleaner is likely designed to convert the input text into phonemes, which are the smallest units of sound in a language.\n",
        "\n",
        "By using the \"phoneme_cleaners\" text cleaner, the TTS model will work with phoneme sequences instead of raw text, enabling it to generate speech that corresponds to the phonetic representation of the input text."
      ],
      "metadata": {
        "id": "qMUopQqDA0c7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will initialize the audio processor using AudioProcessor class, responsible for handling feature extraction from audio and performing audio I/O operations."
      ],
      "metadata": {
        "id": "5PeAMEDceyoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from TTS.utils.audio import AudioProcessor\n",
        "ap = AudioProcessor.init_from_config(config)"
      ],
      "metadata": {
        "id": "R-4Fk3vt5IEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will initialize the tokenizer which is used to convert text to sequences of token IDs. If characters are not defined in the config, default characters are passed to the config."
      ],
      "metadata": {
        "id": "583l9ghThdg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)"
      ],
      "metadata": {
        "id": "KAL4ctJzgEPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will load data samples. Each sample is a list of [text, audio_file_path, speaker_name]."
      ],
      "metadata": {
        "id": "AJ2Cakybht0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from TTS.tts.datasets import load_tts_samples\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    dataset_config,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")"
      ],
      "metadata": {
        "id": "_C_l9xIlhhf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from TTS.tts.models.glow_tts import GlowTTS\n",
        "model = GlowTTS(config, ap, tokenizer, speaker_manager=None)"
      ],
      "metadata": {
        "id": "0zT4eGj1hxl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trainer import Trainer, TrainerArgs\n",
        "trainer = Trainer(\n",
        "    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n",
        ")"
      ],
      "metadata": {
        "id": "7P_0CjpUh5lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86iYqxrriCul",
        "outputId": "8667f486-e9e3-4dbf-d66b-78667827d204"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 0/100\u001b[0m\n",
            " --> tts_train_dir/run-May-16-2023_05+33PM-0000000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*] Pre-computing phonemes...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 4/12969 [00:00<37:50,  5.71it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ɪnstɛd əv weɪtɪŋ ðɛɹ, ɔzwɔld əpɛɹəntli wɛnt æz fɑɹ əweɪ æz hi kʊd ænd bɔɹdɪd ðə fɚst oʊk klɪf bʌs wɪt͡ʃ keɪm əlɔŋ\n",
            " [!] Character '͡' not found in the vocabulary. Discarding it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 2059/12969 [01:20<05:06, 35.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ɪntu ðə “kɹeɪtɚ” dʌɡ aʊt ɪn ðə mɪdəl, pɔɹ ðə spʌnd͡ʒ, wɔɹm wɔtɚ, ðə məlæsɪz, ænd soʊdə dɪzɑlvd ɪn hɑt wɔtɚ.\n",
            " [!] Character '“' not found in the vocabulary. Discarding it.\n",
            "ɪntu ðə “kɹeɪtɚ” dʌɡ aʊt ɪn ðə mɪdəl, pɔɹ ðə spʌnd͡ʒ, wɔɹm wɔtɚ, ðə məlæsɪz, ænd soʊdə dɪzɑlvd ɪn hɑt wɔtɚ.\n",
            " [!] Character '”' not found in the vocabulary. Discarding it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12969/12969 [05:02<00:00, 42.84it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-16 17:38:35) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 12969\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 188\n",
            " | > Min text length: 13\n",
            " | > Avg text length: 100.90014650319993\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 24499.0\n",
            " | > Avg audio length: 144984.29755570978\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[1m   --> STEP: 0/406 -- GLOBAL_STEP: 0\u001b[0m\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 11.77090  (11.77088)\n",
            "     | > loader_time: 3.29410  (3.29406)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 25/406 -- GLOBAL_STEP: 25\u001b[0m\n",
            "     | > loss: 3.65408  (3.52588)\n",
            "     | > log_mle: 0.78891  (0.78805)\n",
            "     | > loss_dur: 2.86517  (2.73782)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 10.31936  (9.37831)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.62110  (0.70223)\n",
            "     | > loader_time: 0.00430  (0.01039)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 50/406 -- GLOBAL_STEP: 50\u001b[0m\n",
            "     | > loss: 3.60657  (3.52123)\n",
            "     | > log_mle: 0.78712  (0.78951)\n",
            "     | > loss_dur: 2.81946  (2.73172)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 10.23356  (9.80654)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.66880  (0.73822)\n",
            "     | > loader_time: 0.00390  (0.01093)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 75/406 -- GLOBAL_STEP: 75\u001b[0m\n",
            "     | > loss: 3.61446  (3.52505)\n",
            "     | > log_mle: 0.79578  (0.78966)\n",
            "     | > loss_dur: 2.81868  (2.73539)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 10.34165  (9.92685)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.31030  (0.80140)\n",
            "     | > loader_time: 0.02380  (0.01136)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 100/406 -- GLOBAL_STEP: 100\u001b[0m\n",
            "     | > loss: 3.51936  (3.52063)\n",
            "     | > log_mle: 0.79471  (0.78973)\n",
            "     | > loss_dur: 2.72465  (2.73090)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 10.13434  (9.96983)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.86240  (0.84007)\n",
            "     | > loader_time: 0.00520  (0.01178)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 125/406 -- GLOBAL_STEP: 125\u001b[0m\n",
            "     | > loss: 3.58194  (3.51758)\n",
            "     | > log_mle: 0.79227  (0.78960)\n",
            "     | > loss_dur: 2.78967  (2.72797)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 10.20621  (9.98981)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.75610  (0.86526)\n",
            "     | > loader_time: 0.00490  (0.01227)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 150/406 -- GLOBAL_STEP: 150\u001b[0m\n",
            "     | > loss: 3.41446  (3.51459)\n",
            "     | > log_mle: 0.79167  (0.78950)\n",
            "     | > loss_dur: 2.62279  (2.72509)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.76636  (9.99344)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.23270  (0.90697)\n",
            "     | > loader_time: 0.02690  (0.01309)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 175/406 -- GLOBAL_STEP: 175\u001b[0m\n",
            "     | > loss: 3.50830  (3.51356)\n",
            "     | > log_mle: 0.78903  (0.78934)\n",
            "     | > loss_dur: 2.71927  (2.72422)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 10.02410  (9.99623)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.22590  (0.92866)\n",
            "     | > loader_time: 0.01190  (0.01298)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 200/406 -- GLOBAL_STEP: 200\u001b[0m\n",
            "     | > loss: 3.44532  (3.51227)\n",
            "     | > log_mle: 0.79167  (0.78924)\n",
            "     | > loss_dur: 2.65365  (2.72303)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.79701  (9.99363)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.84240  (0.96365)\n",
            "     | > loader_time: 0.01250  (0.01329)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 225/406 -- GLOBAL_STEP: 225\u001b[0m\n",
            "     | > loss: 3.49473  (3.51055)\n",
            "     | > log_mle: 0.78703  (0.78925)\n",
            "     | > loss_dur: 2.70770  (2.72130)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.92711  (9.98713)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.11200  (0.99657)\n",
            "     | > loader_time: 0.00990  (0.01349)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 250/406 -- GLOBAL_STEP: 250\u001b[0m\n",
            "     | > loss: 3.55800  (3.50982)\n",
            "     | > log_mle: 0.78961  (0.78929)\n",
            "     | > loss_dur: 2.76839  (2.72054)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 10.02270  (9.97956)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.88130  (1.02662)\n",
            "     | > loader_time: 0.01900  (0.01402)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 275/406 -- GLOBAL_STEP: 275\u001b[0m\n",
            "     | > loss: 3.52008  (3.50886)\n",
            "     | > log_mle: 0.79099  (0.78902)\n",
            "     | > loss_dur: 2.72909  (2.71984)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.90439  (9.96920)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 2.00210  (1.05491)\n",
            "     | > loader_time: 0.03530  (0.01438)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 300/406 -- GLOBAL_STEP: 300\u001b[0m\n",
            "     | > loss: 3.52945  (3.50726)\n",
            "     | > log_mle: 0.78696  (0.78888)\n",
            "     | > loss_dur: 2.74248  (2.71838)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.88191  (9.95545)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.89910  (1.08330)\n",
            "     | > loader_time: 0.02350  (0.01471)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 325/406 -- GLOBAL_STEP: 325\u001b[0m\n",
            "     | > loss: 3.46070  (3.50480)\n",
            "     | > log_mle: 0.78732  (0.78871)\n",
            "     | > loss_dur: 2.67338  (2.71609)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.71985  (9.93834)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 2.49770  (1.11870)\n",
            "     | > loader_time: 0.02600  (0.01502)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 350/406 -- GLOBAL_STEP: 350\u001b[0m\n",
            "     | > loss: 3.43951  (3.50387)\n",
            "     | > log_mle: 0.78425  (0.78857)\n",
            "     | > loss_dur: 2.65526  (2.71530)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.59497  (9.92130)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.75010  (1.15345)\n",
            "     | > loader_time: 0.02370  (0.01530)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 375/406 -- GLOBAL_STEP: 375\u001b[0m\n",
            "     | > loss: 3.52231  (3.50090)\n",
            "     | > log_mle: 0.78176  (0.78833)\n",
            "     | > loss_dur: 2.74056  (2.71257)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.73330  (9.90088)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.35550  (1.18522)\n",
            "     | > loader_time: 0.01480  (0.01552)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 400/406 -- GLOBAL_STEP: 400\u001b[0m\n",
            "     | > loss: 3.49012  (3.49827)\n",
            "     | > log_mle: 0.78363  (0.78821)\n",
            "     | > loss_dur: 2.70650  (2.71007)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.57729  (9.87888)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.97700  (1.21349)\n",
            "     | > loader_time: 0.00550  (0.01568)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 131\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 174\n",
            " | > Min text length: 20\n",
            " | > Avg text length: 100.76335877862596\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 34739.0\n",
            " | > Avg audio length: 144033.41221374046\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.00918 \u001b[0m(+0.00000)\n",
            "     | > avg_loss: 3.45273 \u001b[0m(+0.00000)\n",
            "     | > avg_log_mle: 0.78346 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_dur: 2.66927 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : tts_train_dir/run-May-16-2023_05+33PM-0000000/best_model_406.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 1/100\u001b[0m\n",
            " --> tts_train_dir/run-May-16-2023_05+33PM-0000000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-16 17:47:36) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 12969\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 188\n",
            " | > Min text length: 13\n",
            " | > Avg text length: 100.90014650319993\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 24499.0\n",
            " | > Avg audio length: 144984.29755570978\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[1m   --> STEP: 19/406 -- GLOBAL_STEP: 425\u001b[0m\n",
            "     | > loss: 3.44811  (3.48777)\n",
            "     | > log_mle: 0.78456  (0.78144)\n",
            "     | > loss_dur: 2.66355  (2.70633)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.44169  (9.39360)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.51060  (0.66665)\n",
            "     | > loader_time: 0.00330  (0.00899)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 44/406 -- GLOBAL_STEP: 450\u001b[0m\n",
            "     | > loss: 3.39889  (3.44217)\n",
            "     | > log_mle: 0.78628  (0.78342)\n",
            "     | > loss_dur: 2.61261  (2.65875)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.31497  (9.33772)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.55360  (0.68935)\n",
            "     | > loader_time: 0.02040  (0.01189)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 69/406 -- GLOBAL_STEP: 475\u001b[0m\n",
            "     | > loss: 3.47735  (3.43759)\n",
            "     | > log_mle: 0.79010  (0.78373)\n",
            "     | > loss_dur: 2.68725  (2.65386)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.40473  (9.32960)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.74210  (0.72289)\n",
            "     | > loader_time: 0.01450  (0.01164)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 94/406 -- GLOBAL_STEP: 500\u001b[0m\n",
            "     | > loss: 3.36487  (3.43285)\n",
            "     | > log_mle: 0.78363  (0.78353)\n",
            "     | > loss_dur: 2.58125  (2.64931)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.10779  (9.31337)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.27200  (0.77034)\n",
            "     | > loader_time: 0.01530  (0.01225)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 119/406 -- GLOBAL_STEP: 525\u001b[0m\n",
            "     | > loss: 3.42390  (3.42602)\n",
            "     | > log_mle: 0.77966  (0.78342)\n",
            "     | > loss_dur: 2.64423  (2.64260)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.18396  (9.28603)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.92830  (0.81528)\n",
            "     | > loader_time: 0.02320  (0.01283)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 144/406 -- GLOBAL_STEP: 550\u001b[0m\n",
            "     | > loss: 3.42858  (3.42285)\n",
            "     | > log_mle: 0.78096  (0.78314)\n",
            "     | > loss_dur: 2.64762  (2.63971)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.17306  (9.26034)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.94660  (0.83948)\n",
            "     | > loader_time: 0.00870  (0.01378)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 169/406 -- GLOBAL_STEP: 575\u001b[0m\n",
            "     | > loss: 3.41326  (3.41904)\n",
            "     | > log_mle: 0.78819  (0.78289)\n",
            "     | > loss_dur: 2.62508  (2.63615)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 9.02921  (9.23314)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.09450  (0.88291)\n",
            "     | > loader_time: 0.01730  (0.01426)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 194/406 -- GLOBAL_STEP: 600\u001b[0m\n",
            "     | > loss: 3.38464  (3.41599)\n",
            "     | > log_mle: 0.78332  (0.78255)\n",
            "     | > loss_dur: 2.60132  (2.63344)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 8.92170  (9.20471)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.82490  (0.90630)\n",
            "     | > loader_time: 0.02650  (0.01436)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 219/406 -- GLOBAL_STEP: 625\u001b[0m\n",
            "     | > loss: 3.35425  (3.41075)\n",
            "     | > log_mle: 0.78350  (0.78233)\n",
            "     | > loss_dur: 2.57076  (2.62841)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 8.83628  (9.17112)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.19490  (0.94558)\n",
            "     | > loader_time: 0.01490  (0.01485)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 244/406 -- GLOBAL_STEP: 650\u001b[0m\n",
            "     | > loss: 3.38383  (3.40805)\n",
            "     | > log_mle: 0.77831  (0.78217)\n",
            "     | > loss_dur: 2.60552  (2.62588)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 8.87031  (9.14135)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.07010  (0.97860)\n",
            "     | > loader_time: 0.03170  (0.01554)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 269/406 -- GLOBAL_STEP: 675\u001b[0m\n",
            "     | > loss: 3.32572  (3.40401)\n",
            "     | > log_mle: 0.78265  (0.78177)\n",
            "     | > loss_dur: 2.54307  (2.62225)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 8.66776  (9.10852)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 2.01320  (1.00645)\n",
            "     | > loader_time: 0.02900  (0.01594)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 294/406 -- GLOBAL_STEP: 700\u001b[0m\n",
            "     | > loss: 3.32547  (3.40125)\n",
            "     | > log_mle: 0.77753  (0.78142)\n",
            "     | > loss_dur: 2.54794  (2.61983)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 8.61256  (9.07770)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.77190  (1.03290)\n",
            "     | > loader_time: 0.01100  (0.01660)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 319/406 -- GLOBAL_STEP: 725\u001b[0m\n",
            "     | > loss: 3.35609  (3.39644)\n",
            "     | > log_mle: 0.77262  (0.78107)\n",
            "     | > loss_dur: 2.58346  (2.61536)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 8.64807  (9.04170)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.30210  (1.06125)\n",
            "     | > loader_time: 0.01740  (0.01698)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 344/406 -- GLOBAL_STEP: 750\u001b[0m\n",
            "     | > loss: 3.34065  (3.39407)\n",
            "     | > log_mle: 0.77452  (0.78073)\n",
            "     | > loss_dur: 2.56613  (2.61333)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 8.52347  (9.01022)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.97700  (1.09057)\n",
            "     | > loader_time: 0.01420  (0.01738)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 369/406 -- GLOBAL_STEP: 775\u001b[0m\n",
            "     | > loss: 3.27014  (3.38974)\n",
            "     | > log_mle: 0.77257  (0.78035)\n",
            "     | > loss_dur: 2.49757  (2.60939)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 8.35007  (8.97341)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.98560  (1.12706)\n",
            "     | > loader_time: 0.04050  (0.01759)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 394/406 -- GLOBAL_STEP: 800\u001b[0m\n",
            "     | > loss: 3.30117  (3.38540)\n",
            "     | > log_mle: 0.77933  (0.78002)\n",
            "     | > loss_dur: 2.52184  (2.60537)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 8.29461  (8.93720)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.87950  (1.16052)\n",
            "     | > loader_time: 0.02410  (0.01812)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 131\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 174\n",
            " | > Min text length: 20\n",
            " | > Avg text length: 100.76335877862596\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 34739.0\n",
            " | > Avg audio length: 144033.41221374046\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00888 \u001b[0m(-0.00030)\n",
            "     | > avg_loss:\u001b[92m 3.31518 \u001b[0m(-0.13755)\n",
            "     | > avg_log_mle:\u001b[92m 0.77241 \u001b[0m(-0.01105)\n",
            "     | > avg_loss_dur:\u001b[92m 2.54277 \u001b[0m(-0.12650)\n",
            "\n",
            " > BEST MODEL : tts_train_dir/run-May-16-2023_05+33PM-0000000/best_model_812.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 2/100\u001b[0m\n",
            " --> tts_train_dir/run-May-16-2023_05+33PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-16 17:55:55) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 12969\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 188\n",
            " | > Min text length: 13\n",
            " | > Avg text length: 100.90014650319993\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 24499.0\n",
            " | > Avg audio length: 144984.29755570978\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[1m   --> STEP: 13/406 -- GLOBAL_STEP: 825\u001b[0m\n",
            "     | > loss: 3.23999  (3.36253)\n",
            "     | > log_mle: 0.76409  (0.77083)\n",
            "     | > loss_dur: 2.47590  (2.59170)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 8.02373  (8.27000)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.57430  (0.65838)\n",
            "     | > loader_time: 0.00290  (0.01055)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 38/406 -- GLOBAL_STEP: 850\u001b[0m\n",
            "     | > loss: 3.30448  (3.32271)\n",
            "     | > log_mle: 0.77304  (0.77250)\n",
            "     | > loss_dur: 2.53144  (2.55021)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 8.07801  (8.19287)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.62800  (0.67750)\n",
            "     | > loader_time: 0.00380  (0.01049)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 63/406 -- GLOBAL_STEP: 875\u001b[0m\n",
            "     | > loss: 3.30229  (3.30184)\n",
            "     | > log_mle: 0.76808  (0.77234)\n",
            "     | > loss_dur: 2.53421  (2.52950)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 8.02814  (8.12263)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.66160  (0.71360)\n",
            "     | > loader_time: 0.01340  (0.01069)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 88/406 -- GLOBAL_STEP: 900\u001b[0m\n",
            "     | > loss: 3.22777  (3.29151)\n",
            "     | > log_mle: 0.77175  (0.77189)\n",
            "     | > loss_dur: 2.45601  (2.51961)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 7.83334  (8.06106)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.68040  (0.75350)\n",
            "     | > loader_time: 0.00920  (0.01125)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 113/406 -- GLOBAL_STEP: 925\u001b[0m\n",
            "     | > loss: 3.28608  (3.27561)\n",
            "     | > log_mle: 0.76895  (0.77124)\n",
            "     | > loss_dur: 2.51713  (2.50437)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 7.73412  (7.98328)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.17290  (0.81409)\n",
            "     | > loader_time: 0.01590  (0.01197)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 138/406 -- GLOBAL_STEP: 950\u001b[0m\n",
            "     | > loss: 3.29512  (3.26294)\n",
            "     | > log_mle: 0.76360  (0.77032)\n",
            "     | > loss_dur: 2.53152  (2.49263)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 7.67473  (7.90708)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.71310  (0.83586)\n",
            "     | > loader_time: 0.02130  (0.01280)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 163/406 -- GLOBAL_STEP: 975\u001b[0m\n",
            "     | > loss: 3.19347  (3.25242)\n",
            "     | > log_mle: 0.76362  (0.76940)\n",
            "     | > loss_dur: 2.42985  (2.48302)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 7.39127  (7.83617)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.22760  (0.86686)\n",
            "     | > loader_time: 0.01350  (0.01318)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 188/406 -- GLOBAL_STEP: 1000\u001b[0m\n",
            "     | > loss: 3.29533  (3.24199)\n",
            "     | > log_mle: 0.76160  (0.76850)\n",
            "     | > loss_dur: 2.53373  (2.47349)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 7.48542  (7.76601)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.75170  (0.90132)\n",
            "     | > loader_time: 0.01230  (0.01387)\n",
            "\n",
            "\n",
            " > CHECKPOINT : tts_train_dir/run-May-16-2023_05+33PM-0000000/checkpoint_1000.pth\n",
            "\n",
            "\u001b[1m   --> STEP: 213/406 -- GLOBAL_STEP: 1025\u001b[0m\n",
            "     | > loss: 3.12084  (3.23078)\n",
            "     | > log_mle: 0.75822  (0.76761)\n",
            "     | > loss_dur: 2.36263  (2.46316)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 7.07055  (7.69428)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.94110  (0.93833)\n",
            "     | > loader_time: 0.01830  (0.01448)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 238/406 -- GLOBAL_STEP: 1050\u001b[0m\n",
            "     | > loss: 3.13535  (3.21937)\n",
            "     | > log_mle: 0.76111  (0.76691)\n",
            "     | > loss_dur: 2.37424  (2.45246)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 6.95869  (7.62189)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.50630  (0.97041)\n",
            "     | > loader_time: 0.01780  (0.01522)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 263/406 -- GLOBAL_STEP: 1075\u001b[0m\n",
            "     | > loss: 3.11702  (3.20931)\n",
            "     | > log_mle: 0.75756  (0.76595)\n",
            "     | > loss_dur: 2.35945  (2.44337)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 6.89391  (7.55279)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.65430  (0.99919)\n",
            "     | > loader_time: 0.01330  (0.01577)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 288/406 -- GLOBAL_STEP: 1100\u001b[0m\n",
            "     | > loss: 3.04982  (3.20081)\n",
            "     | > log_mle: 0.75073  (0.76498)\n",
            "     | > loss_dur: 2.29909  (2.43583)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 6.67720  (7.48796)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.33100  (1.03137)\n",
            "     | > loader_time: 0.01780  (0.01630)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 313/406 -- GLOBAL_STEP: 1125\u001b[0m\n",
            "     | > loss: 3.04833  (3.19179)\n",
            "     | > log_mle: 0.75191  (0.76412)\n",
            "     | > loss_dur: 2.29641  (2.42767)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 6.53353  (7.42157)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.45780  (1.06452)\n",
            "     | > loader_time: 0.02100  (0.01669)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 338/406 -- GLOBAL_STEP: 1150\u001b[0m\n",
            "     | > loss: 3.04667  (3.18357)\n",
            "     | > log_mle: 0.75103  (0.76314)\n",
            "     | > loss_dur: 2.29564  (2.42043)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 6.45384  (7.35760)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.10590  (1.09614)\n",
            "     | > loader_time: 0.01280  (0.01725)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 363/406 -- GLOBAL_STEP: 1175\u001b[0m\n",
            "     | > loss: 3.04483  (3.17500)\n",
            "     | > log_mle: 0.74818  (0.76215)\n",
            "     | > loss_dur: 2.29665  (2.41285)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 6.36061  (7.29379)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.76550  (1.12822)\n",
            "     | > loader_time: 0.01600  (0.01760)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 388/406 -- GLOBAL_STEP: 1200\u001b[0m\n",
            "     | > loss: 3.03374  (3.16580)\n",
            "     | > log_mle: 0.74989  (0.76121)\n",
            "     | > loss_dur: 2.28385  (2.40460)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 6.27985  (7.23084)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 2.41870  (1.16387)\n",
            "     | > loader_time: 0.01340  (0.01798)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 131\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 174\n",
            " | > Min text length: 20\n",
            " | > Avg text length: 100.76335877862596\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 34739.0\n",
            " | > Avg audio length: 144033.41221374046\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.01313 \u001b[0m(+0.00426)\n",
            "     | > avg_loss:\u001b[92m 2.99811 \u001b[0m(-0.31707)\n",
            "     | > avg_log_mle:\u001b[92m 0.74328 \u001b[0m(-0.02914)\n",
            "     | > avg_loss_dur:\u001b[92m 2.25483 \u001b[0m(-0.28794)\n",
            "\n",
            " > BEST MODEL : tts_train_dir/run-May-16-2023_05+33PM-0000000/best_model_1218.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 3/100\u001b[0m\n",
            " --> tts_train_dir/run-May-16-2023_05+33PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-16 18:04:25) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 12969\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 188\n",
            " | > Min text length: 13\n",
            " | > Avg text length: 100.90014650319993\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 24499.0\n",
            " | > Avg audio length: 144984.29755570978\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[1m   --> STEP: 7/406 -- GLOBAL_STEP: 1225\u001b[0m\n",
            "     | > loss: 3.04712  (3.11136)\n",
            "     | > log_mle: 0.74373  (0.74251)\n",
            "     | > loss_dur: 2.30339  (2.36885)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 6.26538  (6.31860)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.43660  (0.49502)\n",
            "     | > loader_time: 0.01160  (0.00942)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 32/406 -- GLOBAL_STEP: 1250\u001b[0m\n",
            "     | > loss: 3.09510  (3.05955)\n",
            "     | > log_mle: 0.74397  (0.74435)\n",
            "     | > loss_dur: 2.35113  (2.31521)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 6.28039  (6.24708)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.58790  (0.64060)\n",
            "     | > loader_time: 0.01520  (0.01147)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 57/406 -- GLOBAL_STEP: 1275\u001b[0m\n",
            "     | > loss: 2.98275  (3.03324)\n",
            "     | > log_mle: 0.74196  (0.74373)\n",
            "     | > loss_dur: 2.24079  (2.28950)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.95565  (6.17040)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.14060  (0.70202)\n",
            "     | > loader_time: 0.01590  (0.01230)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 82/406 -- GLOBAL_STEP: 1300\u001b[0m\n",
            "     | > loss: 2.92128  (3.01999)\n",
            "     | > log_mle: 0.73787  (0.74248)\n",
            "     | > loss_dur: 2.18341  (2.27751)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.81497  (6.11435)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.03460  (0.76459)\n",
            "     | > loader_time: 0.03880  (0.01294)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 107/406 -- GLOBAL_STEP: 1325\u001b[0m\n",
            "     | > loss: 2.87432  (3.00259)\n",
            "     | > log_mle: 0.73529  (0.74089)\n",
            "     | > loss_dur: 2.13902  (2.26170)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.73516  (6.04919)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.72630  (0.80109)\n",
            "     | > loader_time: 0.00470  (0.01349)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 132/406 -- GLOBAL_STEP: 1350\u001b[0m\n",
            "     | > loss: 2.94318  (2.98922)\n",
            "     | > log_mle: 0.72536  (0.73906)\n",
            "     | > loss_dur: 2.21782  (2.25016)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.70778  (5.98859)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.80930  (0.82762)\n",
            "     | > loader_time: 0.01550  (0.01370)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 157/406 -- GLOBAL_STEP: 1375\u001b[0m\n",
            "     | > loss: 2.99110  (2.98001)\n",
            "     | > log_mle: 0.72799  (0.73744)\n",
            "     | > loss_dur: 2.26311  (2.24257)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.74253  (5.93816)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.89220  (0.87451)\n",
            "     | > loader_time: 0.01000  (0.01415)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 182/406 -- GLOBAL_STEP: 1400\u001b[0m\n",
            "     | > loss: 2.94022  (2.97113)\n",
            "     | > log_mle: 0.71840  (0.73568)\n",
            "     | > loss_dur: 2.22182  (2.23546)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.60634  (5.88962)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.95100  (0.89926)\n",
            "     | > loader_time: 0.01310  (0.01435)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 207/406 -- GLOBAL_STEP: 1425\u001b[0m\n",
            "     | > loss: 2.90551  (2.96289)\n",
            "     | > log_mle: 0.71797  (0.73405)\n",
            "     | > loss_dur: 2.18754  (2.22884)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.49895  (5.84457)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.04510  (0.93696)\n",
            "     | > loader_time: 0.01600  (0.01524)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 232/406 -- GLOBAL_STEP: 1450\u001b[0m\n",
            "     | > loss: 2.85608  (2.95423)\n",
            "     | > log_mle: 0.71215  (0.73245)\n",
            "     | > loss_dur: 2.14393  (2.22178)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.38286  (5.79995)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.35040  (0.97327)\n",
            "     | > loader_time: 0.01830  (0.01567)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 257/406 -- GLOBAL_STEP: 1475\u001b[0m\n",
            "     | > loss: 2.87150  (2.94775)\n",
            "     | > log_mle: 0.71250  (0.73079)\n",
            "     | > loss_dur: 2.15899  (2.21696)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.37602  (5.76019)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.85040  (1.00043)\n",
            "     | > loader_time: 0.04290  (0.01626)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 282/406 -- GLOBAL_STEP: 1500\u001b[0m\n",
            "     | > loss: 2.84744  (2.94131)\n",
            "     | > log_mle: 0.71183  (0.72904)\n",
            "     | > loss_dur: 2.13560  (2.21227)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.29076  (5.72306)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.17240  (1.02703)\n",
            "     | > loader_time: 0.00810  (0.01663)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 307/406 -- GLOBAL_STEP: 1525\u001b[0m\n",
            "     | > loss: 2.82971  (2.93571)\n",
            "     | > log_mle: 0.70623  (0.72732)\n",
            "     | > loss_dur: 2.12348  (2.20839)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.23421  (5.68900)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.01090  (1.05767)\n",
            "     | > loader_time: 0.02090  (0.01725)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 332/406 -- GLOBAL_STEP: 1550\u001b[0m\n",
            "     | > loss: 2.83257  (2.92949)\n",
            "     | > log_mle: 0.69917  (0.72553)\n",
            "     | > loss_dur: 2.13340  (2.20397)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.23070  (5.65691)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.39240  (1.08860)\n",
            "     | > loader_time: 0.01240  (0.01779)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 357/406 -- GLOBAL_STEP: 1575\u001b[0m\n",
            "     | > loss: 2.80396  (2.92446)\n",
            "     | > log_mle: 0.70238  (0.72370)\n",
            "     | > loss_dur: 2.10159  (2.20076)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.15065  (5.62687)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 2.15600  (1.12021)\n",
            "     | > loader_time: 0.01480  (0.01834)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 382/406 -- GLOBAL_STEP: 1600\u001b[0m\n",
            "     | > loss: 2.85125  (2.91735)\n",
            "     | > log_mle: 0.69554  (0.72188)\n",
            "     | > loss_dur: 2.15571  (2.19547)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.22613  (5.59579)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 2.11570  (1.15125)\n",
            "     | > loader_time: 0.04890  (0.01888)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 131\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 174\n",
            " | > Min text length: 20\n",
            " | > Avg text length: 100.76335877862596\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 34739.0\n",
            " | > Avg audio length: 144033.41221374046\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00971 \u001b[0m(-0.00342)\n",
            "     | > avg_loss:\u001b[92m 2.75287 \u001b[0m(-0.24524)\n",
            "     | > avg_log_mle:\u001b[92m 0.68985 \u001b[0m(-0.05343)\n",
            "     | > avg_loss_dur:\u001b[92m 2.06302 \u001b[0m(-0.19181)\n",
            "\n",
            " > BEST MODEL : tts_train_dir/run-May-16-2023_05+33PM-0000000/best_model_1624.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 4/100\u001b[0m\n",
            " --> tts_train_dir/run-May-16-2023_05+33PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-16 18:12:49) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 12969\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 188\n",
            " | > Min text length: 13\n",
            " | > Avg text length: 100.90014650319993\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 24499.0\n",
            " | > Avg audio length: 144984.29755570978\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[1m   --> STEP: 1/406 -- GLOBAL_STEP: 1625\u001b[0m\n",
            "     | > loss: 2.90296  (2.90296)\n",
            "     | > log_mle: 0.69260  (0.69260)\n",
            "     | > loss_dur: 2.21036  (2.21036)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.28940  (5.28940)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.94060  (0.94061)\n",
            "     | > loader_time: 0.03240  (0.03236)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 26/406 -- GLOBAL_STEP: 1650\u001b[0m\n",
            "     | > loss: 2.77666  (2.83294)\n",
            "     | > log_mle: 0.69429  (0.69573)\n",
            "     | > loss_dur: 2.08237  (2.13721)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.07961  (5.18512)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.62530  (0.62615)\n",
            "     | > loader_time: 0.01570  (0.01193)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 51/406 -- GLOBAL_STEP: 1675\u001b[0m\n",
            "     | > loss: 2.79368  (2.80108)\n",
            "     | > log_mle: 0.68363  (0.69415)\n",
            "     | > loss_dur: 2.11006  (2.10692)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.08514  (5.13650)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.01870  (0.69483)\n",
            "     | > loader_time: 0.02280  (0.01264)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 76/406 -- GLOBAL_STEP: 1700\u001b[0m\n",
            "     | > loss: 2.77449  (2.78854)\n",
            "     | > log_mle: 0.69053  (0.69150)\n",
            "     | > loss_dur: 2.08395  (2.09704)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.08020  (5.10982)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.13220  (0.75693)\n",
            "     | > loader_time: 0.01090  (0.01323)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 101/406 -- GLOBAL_STEP: 1725\u001b[0m\n",
            "     | > loss: 2.78347  (2.77527)\n",
            "     | > log_mle: 0.67479  (0.68860)\n",
            "     | > loss_dur: 2.10868  (2.08667)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 5.08017  (5.07991)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.87520  (0.79369)\n",
            "     | > loader_time: 0.01550  (0.01338)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 126/406 -- GLOBAL_STEP: 1750\u001b[0m\n",
            "     | > loss: 2.64328  (2.75996)\n",
            "     | > log_mle: 0.67140  (0.68558)\n",
            "     | > loss_dur: 1.97188  (2.07438)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.85176  (5.04314)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.86730  (0.82544)\n",
            "     | > loader_time: 0.01150  (0.01363)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 151/406 -- GLOBAL_STEP: 1775\u001b[0m\n",
            "     | > loss: 2.65277  (2.75144)\n",
            "     | > log_mle: 0.66910  (0.68263)\n",
            "     | > loss_dur: 1.98367  (2.06881)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.81191  (5.02354)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.87010  (0.87878)\n",
            "     | > loader_time: 0.00960  (0.01374)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 176/406 -- GLOBAL_STEP: 1800\u001b[0m\n",
            "     | > loss: 2.65801  (2.74168)\n",
            "     | > log_mle: 0.66062  (0.67976)\n",
            "     | > loss_dur: 1.99739  (2.06192)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.84252  (5.00209)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.73650  (0.89971)\n",
            "     | > loader_time: 0.02870  (0.01434)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 201/406 -- GLOBAL_STEP: 1825\u001b[0m\n",
            "     | > loss: 2.61506  (2.73223)\n",
            "     | > log_mle: 0.65975  (0.67692)\n",
            "     | > loss_dur: 1.95531  (2.05531)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.75332  (4.98165)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.15420  (0.94148)\n",
            "     | > loader_time: 0.02230  (0.01501)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 226/406 -- GLOBAL_STEP: 1850\u001b[0m\n",
            "     | > loss: 2.64098  (2.72182)\n",
            "     | > log_mle: 0.64561  (0.67409)\n",
            "     | > loss_dur: 1.99537  (2.04774)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.79208  (4.95971)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.64150  (0.97542)\n",
            "     | > loader_time: 0.04620  (0.01588)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 251/406 -- GLOBAL_STEP: 1875\u001b[0m\n",
            "     | > loss: 2.66485  (2.71330)\n",
            "     | > log_mle: 0.64335  (0.67137)\n",
            "     | > loss_dur: 2.02150  (2.04193)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.81558  (4.93991)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.29750  (0.99997)\n",
            "     | > loader_time: 0.01320  (0.01605)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 276/406 -- GLOBAL_STEP: 1900\u001b[0m\n",
            "     | > loss: 2.57359  (2.70459)\n",
            "     | > log_mle: 0.63933  (0.66850)\n",
            "     | > loss_dur: 1.93426  (2.03608)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.65266  (4.92226)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.36510  (1.03406)\n",
            "     | > loader_time: 0.04100  (0.01657)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 301/406 -- GLOBAL_STEP: 1925\u001b[0m\n",
            "     | > loss: 2.61495  (2.69603)\n",
            "     | > log_mle: 0.62872  (0.66573)\n",
            "     | > loss_dur: 1.98622  (2.03029)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.70721  (4.90502)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.53300  (1.06636)\n",
            "     | > loader_time: 0.02280  (0.01706)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 326/406 -- GLOBAL_STEP: 1950\u001b[0m\n",
            "     | > loss: 2.57027  (2.68647)\n",
            "     | > log_mle: 0.62523  (0.66295)\n",
            "     | > loss_dur: 1.94504  (2.02352)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.73492  (4.88782)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.07400  (1.09784)\n",
            "     | > loader_time: 0.02690  (0.01735)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 351/406 -- GLOBAL_STEP: 1975\u001b[0m\n",
            "     | > loss: 2.54187  (2.67892)\n",
            "     | > log_mle: 0.62275  (0.66011)\n",
            "     | > loss_dur: 1.91912  (2.01881)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.62765  (4.87352)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.36010  (1.12793)\n",
            "     | > loader_time: 0.02720  (0.01777)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 376/406 -- GLOBAL_STEP: 2000\u001b[0m\n",
            "     | > loss: 2.53049  (2.66942)\n",
            "     | > log_mle: 0.61861  (0.65728)\n",
            "     | > loss_dur: 1.91187  (2.01214)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.61469  (4.85710)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.96920  (1.16101)\n",
            "     | > loader_time: 0.02540  (0.01823)\n",
            "\n",
            "\n",
            " > CHECKPOINT : tts_train_dir/run-May-16-2023_05+33PM-0000000/checkpoint_2000.pth\n",
            "\n",
            "\u001b[1m   --> STEP: 401/406 -- GLOBAL_STEP: 2025\u001b[0m\n",
            "     | > loss: 2.51112  (2.66014)\n",
            "     | > log_mle: 0.61164  (0.65454)\n",
            "     | > loss_dur: 1.89948  (2.00560)\n",
            "     | > amp_scaler: 16384.00000  (16424.85786)\n",
            "     | > grad_norm: 4.59921  (4.82928)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.93210  (1.18565)\n",
            "     | > loader_time: 0.00600  (0.01850)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 131\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 174\n",
            " | > Min text length: 20\n",
            " | > Avg text length: 100.76335877862596\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 34739.0\n",
            " | > Avg audio length: 144033.41221374046\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.01074 \u001b[0m(+0.00103)\n",
            "     | > avg_loss:\u001b[92m 2.43641 \u001b[0m(-0.31646)\n",
            "     | > avg_log_mle:\u001b[92m 0.61022 \u001b[0m(-0.07963)\n",
            "     | > avg_loss_dur:\u001b[92m 1.82619 \u001b[0m(-0.23683)\n",
            "\n",
            " > BEST MODEL : tts_train_dir/run-May-16-2023_05+33PM-0000000/best_model_2030.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 5/100\u001b[0m\n",
            " --> tts_train_dir/run-May-16-2023_05+33PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-16 18:21:29) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 12969\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 188\n",
            " | > Min text length: 13\n",
            " | > Avg text length: 100.90014650319993\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 24499.0\n",
            " | > Avg audio length: 144984.29755570978\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[1m   --> STEP: 20/406 -- GLOBAL_STEP: 2050\u001b[0m\n",
            "     | > loss: 2.43312  (2.52465)\n",
            "     | > log_mle: 0.63105  (0.62374)\n",
            "     | > loss_dur: 1.80207  (1.90090)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.48053  (4.61263)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.65520  (0.67430)\n",
            "     | > loader_time: 0.01330  (0.01126)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 45/406 -- GLOBAL_STEP: 2075\u001b[0m\n",
            "     | > loss: 2.46401  (2.48640)\n",
            "     | > log_mle: 0.61868  (0.62113)\n",
            "     | > loss_dur: 1.84533  (1.86527)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.55479  (4.56645)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.66150  (0.69995)\n",
            "     | > loader_time: 0.01340  (0.01353)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 70/406 -- GLOBAL_STEP: 2100\u001b[0m\n",
            "     | > loss: 2.41545  (2.47478)\n",
            "     | > log_mle: 0.59952  (0.61649)\n",
            "     | > loss_dur: 1.81593  (1.85829)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.45098  (4.55244)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.06180  (0.74339)\n",
            "     | > loader_time: 0.00540  (0.01326)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 95/406 -- GLOBAL_STEP: 2125\u001b[0m\n",
            "     | > loss: 2.37266  (2.46192)\n",
            "     | > log_mle: 0.59836  (0.61216)\n",
            "     | > loss_dur: 1.77431  (1.84976)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.43713  (4.53918)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.17560  (0.80687)\n",
            "     | > loader_time: 0.02310  (0.01330)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 120/406 -- GLOBAL_STEP: 2150\u001b[0m\n",
            "     | > loss: 2.37626  (2.44642)\n",
            "     | > log_mle: 0.58476  (0.60809)\n",
            "     | > loss_dur: 1.79150  (1.83833)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.39810  (4.51412)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.68950  (0.83758)\n",
            "     | > loader_time: 0.00490  (0.01337)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 145/406 -- GLOBAL_STEP: 2175\u001b[0m\n",
            "     | > loss: 2.29048  (2.43334)\n",
            "     | > log_mle: 0.58235  (0.60386)\n",
            "     | > loss_dur: 1.70812  (1.82947)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.28005  (4.49278)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.52750  (0.87898)\n",
            "     | > loader_time: 0.02570  (0.01359)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 170/406 -- GLOBAL_STEP: 2200\u001b[0m\n",
            "     | > loss: 2.35035  (2.42180)\n",
            "     | > log_mle: 0.57153  (0.60006)\n",
            "     | > loss_dur: 1.77883  (1.82174)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.39786  (4.47650)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.03510  (0.91167)\n",
            "     | > loader_time: 0.02400  (0.01428)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 195/406 -- GLOBAL_STEP: 2225\u001b[0m\n",
            "     | > loss: 2.31763  (2.41101)\n",
            "     | > log_mle: 0.57720  (0.59622)\n",
            "     | > loss_dur: 1.74043  (1.81479)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.33393  (4.46199)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.80380  (0.95237)\n",
            "     | > loader_time: 0.02800  (0.01483)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 220/406 -- GLOBAL_STEP: 2250\u001b[0m\n",
            "     | > loss: 2.31701  (2.39835)\n",
            "     | > log_mle: 0.55240  (0.59254)\n",
            "     | > loss_dur: 1.76461  (1.80582)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.31229  (4.44530)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.00570  (0.97375)\n",
            "     | > loader_time: 0.02890  (0.01528)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 245/406 -- GLOBAL_STEP: 2275\u001b[0m\n",
            "     | > loss: 2.27025  (2.38721)\n",
            "     | > log_mle: 0.56035  (0.58894)\n",
            "     | > loss_dur: 1.70990  (1.79826)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.25333  (4.42942)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.27450  (1.00985)\n",
            "     | > loader_time: 0.01570  (0.01584)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 270/406 -- GLOBAL_STEP: 2300\u001b[0m\n",
            "     | > loss: 2.26529  (2.37575)\n",
            "     | > log_mle: 0.54895  (0.58533)\n",
            "     | > loss_dur: 1.71635  (1.79041)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.26683  (4.41459)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.06250  (1.04195)\n",
            "     | > loader_time: 0.02340  (0.01657)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 295/406 -- GLOBAL_STEP: 2325\u001b[0m\n",
            "     | > loss: 2.20465  (2.36409)\n",
            "     | > log_mle: 0.54326  (0.58178)\n",
            "     | > loss_dur: 1.66139  (1.78231)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.14745  (4.39880)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.21350  (1.07094)\n",
            "     | > loader_time: 0.01610  (0.01711)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 320/406 -- GLOBAL_STEP: 2350\u001b[0m\n",
            "     | > loss: 2.18219  (2.35156)\n",
            "     | > log_mle: 0.54120  (0.57834)\n",
            "     | > loss_dur: 1.64100  (1.77322)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.10527  (4.38026)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.56780  (1.09909)\n",
            "     | > loader_time: 0.03130  (0.01770)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 345/406 -- GLOBAL_STEP: 2375\u001b[0m\n",
            "     | > loss: 2.17122  (2.34089)\n",
            "     | > log_mle: 0.52109  (0.57478)\n",
            "     | > loss_dur: 1.65014  (1.76611)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.06606  (4.36537)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.36670  (1.12715)\n",
            "     | > loader_time: 0.01960  (0.01823)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 370/406 -- GLOBAL_STEP: 2400\u001b[0m\n",
            "     | > loss: 2.15355  (2.32884)\n",
            "     | > log_mle: 0.51532  (0.57132)\n",
            "     | > loss_dur: 1.63823  (1.75753)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.08581  (4.34660)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.26520  (1.15621)\n",
            "     | > loader_time: 0.01770  (0.01871)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 395/406 -- GLOBAL_STEP: 2425\u001b[0m\n",
            "     | > loss: 2.12274  (2.31652)\n",
            "     | > log_mle: 0.51159  (0.56799)\n",
            "     | > loss_dur: 1.61115  (1.74853)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.99892  (4.32643)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.54120  (1.18573)\n",
            "     | > loader_time: 0.01890  (0.01883)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 131\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 174\n",
            " | > Min text length: 20\n",
            " | > Avg text length: 100.76335877862596\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 34739.0\n",
            " | > Avg audio length: 144033.41221374046\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.01387 \u001b[0m(+0.00313)\n",
            "     | > avg_loss:\u001b[92m 2.04308 \u001b[0m(-0.39333)\n",
            "     | > avg_log_mle:\u001b[92m 0.51466 \u001b[0m(-0.09556)\n",
            "     | > avg_loss_dur:\u001b[92m 1.52842 \u001b[0m(-0.29777)\n",
            "\n",
            " > BEST MODEL : tts_train_dir/run-May-16-2023_05+33PM-0000000/best_model_2436.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 6/100\u001b[0m\n",
            " --> tts_train_dir/run-May-16-2023_05+33PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-16 18:30:04) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 12969\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 188\n",
            " | > Min text length: 13\n",
            " | > Avg text length: 100.90014650319993\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 24499.0\n",
            " | > Avg audio length: 144984.29755570978\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[1m   --> STEP: 14/406 -- GLOBAL_STEP: 2450\u001b[0m\n",
            "     | > loss: 2.14259  (2.14212)\n",
            "     | > log_mle: 0.52022  (0.53317)\n",
            "     | > loss_dur: 1.62237  (1.60895)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 4.07272  (4.03119)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.50390  (0.72681)\n",
            "     | > loader_time: 0.01850  (0.01334)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 39/406 -- GLOBAL_STEP: 2475\u001b[0m\n",
            "     | > loss: 2.08823  (2.10643)\n",
            "     | > log_mle: 0.52820  (0.53091)\n",
            "     | > loss_dur: 1.56003  (1.57551)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.98090  (3.97942)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.49010  (0.71204)\n",
            "     | > loader_time: 0.00890  (0.01240)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 64/406 -- GLOBAL_STEP: 2500\u001b[0m\n",
            "     | > loss: 1.94730  (2.08417)\n",
            "     | > log_mle: 0.52504  (0.52642)\n",
            "     | > loss_dur: 1.42226  (1.55775)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.68343  (3.93746)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.65290  (0.74369)\n",
            "     | > loader_time: 0.00460  (0.01199)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 89/406 -- GLOBAL_STEP: 2525\u001b[0m\n",
            "     | > loss: 2.04665  (2.07108)\n",
            "     | > log_mle: 0.50815  (0.52176)\n",
            "     | > loss_dur: 1.53850  (1.54932)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.84882  (3.91000)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.78860  (0.77357)\n",
            "     | > loader_time: 0.01470  (0.01270)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 114/406 -- GLOBAL_STEP: 2550\u001b[0m\n",
            "     | > loss: 2.00480  (2.05381)\n",
            "     | > log_mle: 0.49066  (0.51723)\n",
            "     | > loss_dur: 1.51414  (1.53659)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.68946  (3.87055)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.44630  (0.82288)\n",
            "     | > loader_time: 0.01470  (0.01341)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 139/406 -- GLOBAL_STEP: 2575\u001b[0m\n",
            "     | > loss: 2.00469  (2.04074)\n",
            "     | > log_mle: 0.49056  (0.51287)\n",
            "     | > loss_dur: 1.51414  (1.52787)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.70530  (3.83678)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.94090  (0.85800)\n",
            "     | > loader_time: 0.01700  (0.01393)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 164/406 -- GLOBAL_STEP: 2600\u001b[0m\n",
            "     | > loss: 1.95283  (2.02890)\n",
            "     | > log_mle: 0.49110  (0.50900)\n",
            "     | > loss_dur: 1.46174  (1.51990)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.58650  (3.80587)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.34240  (0.89032)\n",
            "     | > loader_time: 0.02790  (0.01445)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 189/406 -- GLOBAL_STEP: 2625\u001b[0m\n",
            "     | > loss: 1.98385  (2.01793)\n",
            "     | > log_mle: 0.47305  (0.50526)\n",
            "     | > loss_dur: 1.51080  (1.51267)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.66050  (3.77755)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.98150  (0.92418)\n",
            "     | > loader_time: 0.02370  (0.01485)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 214/406 -- GLOBAL_STEP: 2650\u001b[0m\n",
            "     | > loss: 1.87150  (2.00553)\n",
            "     | > log_mle: 0.47576  (0.50189)\n",
            "     | > loss_dur: 1.39574  (1.50364)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.39593  (3.74664)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.55750  (0.96187)\n",
            "     | > loader_time: 0.02000  (0.01533)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 239/406 -- GLOBAL_STEP: 2675\u001b[0m\n",
            "     | > loss: 1.93451  (1.99473)\n",
            "     | > log_mle: 0.45714  (0.49861)\n",
            "     | > loss_dur: 1.47737  (1.49612)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.57645  (3.71870)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.56660  (0.98334)\n",
            "     | > loader_time: 0.01320  (0.01569)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 264/406 -- GLOBAL_STEP: 2700\u001b[0m\n",
            "     | > loss: 1.90406  (1.98432)\n",
            "     | > log_mle: 0.46374  (0.49547)\n",
            "     | > loss_dur: 1.44032  (1.48886)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.46172  (3.69201)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.09590  (1.01568)\n",
            "     | > loader_time: 0.02090  (0.01591)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 289/406 -- GLOBAL_STEP: 2725\u001b[0m\n",
            "     | > loss: 1.82525  (1.97438)\n",
            "     | > log_mle: 0.45763  (0.49238)\n",
            "     | > loss_dur: 1.36762  (1.48200)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.34223  (3.66609)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.75310  (1.04672)\n",
            "     | > loader_time: 0.02510  (0.01645)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 314/406 -- GLOBAL_STEP: 2750\u001b[0m\n",
            "     | > loss: 1.80206  (1.96446)\n",
            "     | > log_mle: 0.44898  (0.48958)\n",
            "     | > loss_dur: 1.35307  (1.47488)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.22385  (3.63890)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.15730  (1.07981)\n",
            "     | > loader_time: 0.02880  (0.01677)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 339/406 -- GLOBAL_STEP: 2775\u001b[0m\n",
            "     | > loss: 1.83987  (1.95550)\n",
            "     | > log_mle: 0.45013  (0.48673)\n",
            "     | > loss_dur: 1.38975  (1.46877)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.31818  (3.61532)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.58850  (1.10825)\n",
            "     | > loader_time: 0.02950  (0.01756)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 364/406 -- GLOBAL_STEP: 2800\u001b[0m\n",
            "     | > loss: 1.82659  (1.94657)\n",
            "     | > log_mle: 0.45100  (0.48392)\n",
            "     | > loss_dur: 1.37559  (1.46265)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.25960  (3.59126)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.40030  (1.13782)\n",
            "     | > loader_time: 0.04120  (0.01782)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 389/406 -- GLOBAL_STEP: 2825\u001b[0m\n",
            "     | > loss: 1.77931  (1.93692)\n",
            "     | > log_mle: 0.43504  (0.48132)\n",
            "     | > loss_dur: 1.34427  (1.45560)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.11314  (3.56615)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 2.29910  (1.17124)\n",
            "     | > loader_time: 0.04510  (0.01804)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 131\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 174\n",
            " | > Min text length: 20\n",
            " | > Avg text length: 100.76335877862596\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 34739.0\n",
            " | > Avg audio length: 144033.41221374046\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.00996 \u001b[0m(-0.00391)\n",
            "     | > avg_loss:\u001b[92m 1.72562 \u001b[0m(-0.31746)\n",
            "     | > avg_log_mle:\u001b[92m 0.44169 \u001b[0m(-0.07297)\n",
            "     | > avg_loss_dur:\u001b[92m 1.28393 \u001b[0m(-0.24449)\n",
            "\n",
            " > BEST MODEL : tts_train_dir/run-May-16-2023_05+33PM-0000000/best_model_2842.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 7/100\u001b[0m\n",
            " --> tts_train_dir/run-May-16-2023_05+33PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-16 18:38:33) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 3 not found characters:\n",
            "\t| > ͡\n",
            "\t| > “\n",
            "\t| > ”\n",
            "| > Number of instances : 12969\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 188\n",
            " | > Min text length: 13\n",
            " | > Avg text length: 100.90014650319993\n",
            " | \n",
            " | > Max audio length: 222643.0\n",
            " | > Min audio length: 24499.0\n",
            " | > Avg audio length: 144984.29755570978\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[1m   --> STEP: 8/406 -- GLOBAL_STEP: 2850\u001b[0m\n",
            "     | > loss: 1.81816  (1.82325)\n",
            "     | > log_mle: 0.46040  (0.46742)\n",
            "     | > loss_dur: 1.35776  (1.35583)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.15904  (3.20936)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.52400  (0.52279)\n",
            "     | > loader_time: 0.00840  (0.00931)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 33/406 -- GLOBAL_STEP: 2875\u001b[0m\n",
            "     | > loss: 1.79396  (1.78294)\n",
            "     | > log_mle: 0.45811  (0.46281)\n",
            "     | > loss_dur: 1.33585  (1.32012)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.19144  (3.16200)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.61600  (0.63241)\n",
            "     | > loader_time: 0.00520  (0.00993)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 58/406 -- GLOBAL_STEP: 2900\u001b[0m\n",
            "     | > loss: 1.79182  (1.76837)\n",
            "     | > log_mle: 0.45139  (0.45966)\n",
            "     | > loss_dur: 1.34043  (1.30871)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.14062  (3.12631)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.59670  (0.68658)\n",
            "     | > loader_time: 0.01360  (0.01064)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 83/406 -- GLOBAL_STEP: 2925\u001b[0m\n",
            "     | > loss: 1.75100  (1.75809)\n",
            "     | > log_mle: 0.43907  (0.45562)\n",
            "     | > loss_dur: 1.31193  (1.30247)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.07743  (3.10203)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.41400  (0.74871)\n",
            "     | > loader_time: 0.01320  (0.01143)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 108/406 -- GLOBAL_STEP: 2950\u001b[0m\n",
            "     | > loss: 1.68626  (1.74482)\n",
            "     | > log_mle: 0.43442  (0.45170)\n",
            "     | > loss_dur: 1.25184  (1.29312)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 3.02306  (3.07413)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.80010  (0.80601)\n",
            "     | > loader_time: 0.01070  (0.01244)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 133/406 -- GLOBAL_STEP: 2975\u001b[0m\n",
            "     | > loss: 1.68953  (1.73525)\n",
            "     | > log_mle: 0.42708  (0.44771)\n",
            "     | > loss_dur: 1.26245  (1.28754)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 2.97731  (3.05008)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.76230  (0.83262)\n",
            "     | > loader_time: 0.00990  (0.01256)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 158/406 -- GLOBAL_STEP: 3000\u001b[0m\n",
            "     | > loss: 1.68971  (1.72854)\n",
            "     | > log_mle: 0.41665  (0.44445)\n",
            "     | > loss_dur: 1.27306  (1.28409)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 2.92779  (3.03361)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.31300  (0.88161)\n",
            "     | > loader_time: 0.02090  (0.01348)\n",
            "\n",
            "\n",
            " > CHECKPOINT : tts_train_dir/run-May-16-2023_05+33PM-0000000/checkpoint_3000.pth\n",
            "\n",
            "\u001b[1m   --> STEP: 183/406 -- GLOBAL_STEP: 3025\u001b[0m\n",
            "     | > loss: 1.65633  (1.72117)\n",
            "     | > log_mle: 0.42008  (0.44144)\n",
            "     | > loss_dur: 1.23625  (1.27973)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 2.84738  (3.01495)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.90440  (0.90727)\n",
            "     | > loader_time: 0.03550  (0.01397)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 208/406 -- GLOBAL_STEP: 3050\u001b[0m\n",
            "     | > loss: 1.60585  (1.71389)\n",
            "     | > log_mle: 0.40556  (0.43857)\n",
            "     | > loss_dur: 1.20029  (1.27532)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 2.82603  (2.99692)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.98810  (0.94064)\n",
            "     | > loader_time: 0.01280  (0.01442)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 233/406 -- GLOBAL_STEP: 3075\u001b[0m\n",
            "     | > loss: 1.65383  (1.70639)\n",
            "     | > log_mle: 0.41209  (0.43584)\n",
            "     | > loss_dur: 1.24174  (1.27055)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 2.75874  (2.97747)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 0.98550  (0.97588)\n",
            "     | > loader_time: 0.01830  (0.01495)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 258/406 -- GLOBAL_STEP: 3100\u001b[0m\n",
            "     | > loss: 1.68020  (1.70040)\n",
            "     | > log_mle: 0.40873  (0.43327)\n",
            "     | > loss_dur: 1.27148  (1.26713)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 2.90892  (2.96132)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 2.01650  (1.00462)\n",
            "     | > loader_time: 0.05240  (0.01571)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 283/406 -- GLOBAL_STEP: 3125\u001b[0m\n",
            "     | > loss: 1.62122  (1.69384)\n",
            "     | > log_mle: 0.39890  (0.43073)\n",
            "     | > loss_dur: 1.22232  (1.26311)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 2.78041  (2.94515)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.73020  (1.03206)\n",
            "     | > loader_time: 0.02240  (0.01619)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 308/406 -- GLOBAL_STEP: 3150\u001b[0m\n",
            "     | > loss: 1.59480  (1.68779)\n",
            "     | > log_mle: 0.39904  (0.42845)\n",
            "     | > loss_dur: 1.19576  (1.25934)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 2.70963  (2.92823)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.49040  (1.06279)\n",
            "     | > loader_time: 0.02740  (0.01653)\n",
            "\n",
            "\n",
            "\u001b[1m   --> STEP: 333/406 -- GLOBAL_STEP: 3175\u001b[0m\n",
            "     | > loss: 1.64637  (1.68158)\n",
            "     | > log_mle: 0.39072  (0.42614)\n",
            "     | > loss_dur: 1.25564  (1.25543)\n",
            "     | > amp_scaler: 16384.00000  (16384.00000)\n",
            "     | > grad_norm: 2.82665  (2.91270)\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 1.52490  (1.09332)\n",
            "     | > loader_time: 0.00810  (0.01707)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard\n",
        "!tensorboard --logdir=tts_train_dir"
      ],
      "metadata": {
        "id": "zO5SHGMdiEJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "output_path = \"tts_train_dir\"\n",
        "ckpts = sorted([f for f in glob.glob(output_path+\"/*/*.pth\")])\n",
        "configs = sorted([f for f in glob.glob(output_path+\"/*/*.json\")])"
      ],
      "metadata": {
        "id": "tkT-9AGYiMF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tts --text \"Text for TTS\" \\\n",
        "      --model_path $test_ckpt \\\n",
        "      --config_path $test_config \\\n",
        "      --out_path out.wav"
      ],
      "metadata": {
        "id": "gIvjA16NiPJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "IPython.display.Audio(\"out.wav\")"
      ],
      "metadata": {
        "id": "BA8OhCk4iSBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cuYMUd3uifj0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}